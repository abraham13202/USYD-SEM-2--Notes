{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc4122b0",
   "metadata": {},
   "source": [
    "# COMP4318/5318 Assignment 2: Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9f508e",
   "metadata": {},
   "source": [
    "### Group number: 44\n",
    "### Student 1 SID: 550217239\n",
    "### Student 2 SID: 550232025\n",
    "### Student 3 SID: 550332875\n",
    "### Student 4 SID: 550300357"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24652d1",
   "metadata": {},
   "source": [
    "This template notebook includes code to load the  dataset and a skeleton for the main sections that should be included in the notebook. Please stick to this struture for your submitted notebook.\n",
    "\n",
    "Please focus on making your code clear, with appropriate variable names and whitespace. Include comments and markdown text to aid the readability of your code where relevant. See the specification and marking criteria in the associated specification to guide you when completing your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1144e1",
   "metadata": {},
   "source": [
    "## Setup and dependencies\n",
    "Please use this section to list and set up all your required libraries/dependencies and your plotting environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ab0507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc35bce",
   "metadata": {},
   "source": [
    "## 1. Data loading, exploration, and preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7495a12e-9b9d-48f8-9d2a-abbd01c5a594",
   "metadata": {},
   "source": [
    "Code to load the dataset is provided in the following cell. Please proceed with your data exploration and preprocessing in the remainder of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a991e631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset training and test sets as numpy arrays\n",
    "# assuming Assignment2Data folder is present in the same directory \n",
    "# as the notebook\n",
    "X_train = np.load('Assignment2Data/X_train.npy')\n",
    "y_train = np.load('Assignment2Data/y_train.npy')\n",
    "X_test = np.load('Assignment2Data/X_test.npy')\n",
    "y_test = np.load('Assignment2Data/y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06a615f-5fc5-4e24-a787-4f3ee5ee4a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27b199d-658e-4169-a8e4-b1eb5ba9157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train dtype:\", X_train.dtype)\n",
    "print(\"y_train dtype:\", y_train.dtype)\n",
    "print(\"X_train min/max:\", X_train.min(), X_train.max())\n",
    "print(\"y_train unique labels:\", np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f5548a-28d5-42ff-8e27-67a02bb26582",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First training sample (flattened):\", X_train[0].flatten()[:50])  # first 50 values\n",
    "print(\"Label:\", y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66b0df7-90f2-437b-9a23-fcd79e8ed57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[0])\n",
    "plt.title(f\"Label: {y_train[0]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079eb164-1dde-444e-8333-700ace5b35c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean pixel value:\", X_train.mean())\n",
    "print(\"Std pixel value:\", X_train.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0befa3-a14f-4818-b13d-055acb98a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(10), counts)\n",
    "plt.xticks(range(10), class_names, rotation=45)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of samples')\n",
    "plt.title('Training Set Class Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Class distribution:\")\n",
    "for i, count in enumerate(counts):\n",
    "    print(f\"{class_names[i]}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12b84bc-d3df-47e2-a90d-e420330fafdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images from each class\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(10):\n",
    "    # Find first image of each class\n",
    "    idx = np.where(y_train == i)[0][0]\n",
    "    axes[i].imshow(X_train[idx])\n",
    "    axes[i].set_title(class_names[i])\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8fa8c1-0dba-4fc9-98eb-c93378b9d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize multiple samples from a single class to see variation\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Let's look at cats (class 3)\n",
    "cat_indices = np.where(y_train == 3)[0][:10]\n",
    "\n",
    "for i, idx in enumerate(cat_indices):\n",
    "    axes[i].imshow(X_train[idx])\n",
    "    axes[i].set_title(f'Cat #{i+1}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Variation within \"cat\" class')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1be6788-a3b4-4b3a-86cd-373f4b2aae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze pixel intensity distribution across channels\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, color in enumerate(['Red', 'Green', 'Blue']):\n",
    "    axes[i].hist(X_train[:1000, :, :, i].flatten(), bins=50, alpha=0.7)\n",
    "    axes[i].set_title(f'{color} Channel Distribution (first 1000 images)')\n",
    "    axes[i].set_xlabel('Pixel Value')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da119541-1e68-4f56-9a99-9db50077afa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean image per class\n",
    "mean_images = []\n",
    "for i in range(10):\n",
    "    class_images = X_train[y_train == i]\n",
    "    mean_img = class_images.mean(axis=0).astype(np.uint8)\n",
    "    mean_images.append(mean_img)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(10):\n",
    "    axes[i].imshow(mean_images[i])\n",
    "    axes[i].set_title(f'Mean {class_names[i]}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c97ca0-15b8-4101-b6b5-b76431c3ca17",
   "metadata": {},
   "source": [
    "### Examples of preprocessed data\n",
    "Please print/display some examples of your preprocessed data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741d3d74-19a6-42bb-b5eb-b62da0e0b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Normalize pixel values to [0, 1]\n",
    "X_train_normalized = X_train.astype('float32') / 255.0\n",
    "X_test_normalized = X_test.astype('float32') / 255.0\n",
    "\n",
    "print(\"After normalization:\")\n",
    "print(\"X_train min/max:\", X_train_normalized.min(), X_train_normalized.max())\n",
    "print(\"X_train mean/std:\", X_train_normalized.mean(), X_train_normalized.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108d5620-b2ac-4118-9e95-a30b447b0994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. One-hot encode labels for neural networks\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_categorical = to_categorical(y_train, num_classes=10)\n",
    "y_test_categorical = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "print(\"Original label shape:\", y_train.shape)\n",
    "print(\"One-hot encoded shape:\", y_train_categorical.shape)\n",
    "print(\"Example - original label:\", y_train[0])\n",
    "print(\"Example - one-hot encoded:\", y_train_categorical[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d31bab1-0534-4098-9cb7-b5ed3af437be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create validation split for hyperparameter tuning\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_normalized, y_train, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_train  # maintain class distribution\n",
    ")\n",
    "\n",
    "# Also create categorical versions for neural networks\n",
    "y_train_final_cat, y_val_cat = train_test_split(\n",
    "    y_train_categorical, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "print(\"Training set:\", X_train_final.shape, y_train_final.shape)\n",
    "print(\"Validation set:\", X_val.shape, y_val.shape)\n",
    "print(\"Test set:\", X_test_normalized.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1500e86-ae9e-49fb-b8ca-a17ec3c0b0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display preprocessed examples\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "axes[0].imshow(X_train[0])\n",
    "axes[0].set_title('Original')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(X_train_normalized[0])\n",
    "axes[1].set_title('Normalized')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Show the difference is subtle visually\n",
    "axes[2].imshow(X_train_final[0])\n",
    "axes[2].set_title('Final Preprocessed')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81bb2b0",
   "metadata": {},
   "source": [
    "## 2. Algorithm design and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496a52cf-3001-4e6b-ae3d-ad296c5588d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these imports to your setup section\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c873320",
   "metadata": {},
   "source": [
    "### Algorithm of choice from first six weeks of course- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b2aef2-5b68-4525-b091-ce779e88d792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Random Forest, we need to flatten the images\n",
    "X_train_flat = X_train_final.reshape(X_train_final.shape[0], -1)\n",
    "X_val_flat = X_val.reshape(X_val.shape[0], -1)\n",
    "X_test_flat = X_test_normalized.reshape(X_test_normalized.shape[0], -1)\n",
    "\n",
    "print(\"Flattened shape:\", X_train_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fc170c-87be-4b6b-bdb9-b37e2da9c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple Random Forest model to test\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "print(\"Training Random Forest...\")\n",
    "start_time = time.time()\n",
    "rf_model.fit(X_train_flat, y_train_final)\n",
    "rf_time = time.time() - start_time\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_val_pred = rf_model.predict(X_val_flat)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"Training time: {rf_time:.2f} seconds\")\n",
    "print(f\"Validation accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b895ee03",
   "metadata": {},
   "source": [
    "### Fully connected neural network- Multilayer Perceptron(MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285ab2c8-4544-49f2-a853-d3156c81eadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(input_shape, hidden_layers=[128, 64], dropout_rate=0.3, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Create a fully connected MLP model\n",
    "    \n",
    "    Args:\n",
    "        input_shape: shape of flattened input\n",
    "        hidden_layers: list of units in each hidden layer\n",
    "        dropout_rate: dropout rate for regularization\n",
    "        learning_rate: learning rate for optimizer\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(layers.Input(shape=input_shape))\n",
    "    \n",
    "    # Hidden layers\n",
    "    for units in hidden_layers:\n",
    "        model.add(layers.Dense(units, activation='relu'))\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and test MLP\n",
    "mlp_model = create_mlp(input_shape=(3072,), hidden_layers=[256, 128], dropout_rate=0.3)\n",
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b648aee-ee2f-49dc-b75d-c95080d0c94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MLP with early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "print(\"Training MLP...\")\n",
    "start_time = time.time()\n",
    "\n",
    "history_mlp = mlp_model.fit(\n",
    "    X_train_flat, y_train_final_cat,\n",
    "    validation_data=(X_val_flat, y_val_cat),\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "mlp_time = time.time() - start_time\n",
    "print(f\"Training time: {mlp_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d332c4dc-b298-4de0-8134-bb8fee82ec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_mlp.history['loss'], label='Train Loss')\n",
    "plt.plot(history_mlp.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('MLP Training and Validation Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_mlp.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history_mlp.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('MLP Training and Validation Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Validation accuracy\n",
    "val_loss, val_acc = mlp_model.evaluate(X_val_flat, y_val_cat, verbose=0)\n",
    "print(f\"MLP Validation Accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aa570f",
   "metadata": {},
   "source": [
    "### Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29445abb-3c3d-49c0-a098-45c7ec422660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(input_shape=(32, 32, 3), filters=[32, 64], kernel_size=3, \n",
    "               dense_units=128, dropout_rate=0.3, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Creating a CNN model\n",
    "    \n",
    "    Args:\n",
    "        input_shape: shape of input images\n",
    "        filters: list of filters for each conv layer\n",
    "        kernel_size: size of convolutional kernel\n",
    "        dense_units: units in dense layer\n",
    "        dropout_rate: dropout rate\n",
    "        learning_rate: learning rate\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # First convolutional block\n",
    "    model.add(layers.Conv2D(filters[0], (kernel_size, kernel_size), \n",
    "                            activation='relu', padding='same', \n",
    "                            input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Second convolutional block\n",
    "    model.add(layers.Conv2D(filters[1], (kernel_size, kernel_size), \n",
    "                            activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Flatten and dense layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(dense_units, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    # Compile\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and test CNN\n",
    "cnn_model = create_cnn(filters=[32, 64], dense_units=128, dropout_rate=0.3)\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cd2503-0e10-4260-b32a-0955d381bef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN\n",
    "print(\"Training CNN...\")\n",
    "start_time = time.time()\n",
    "\n",
    "history_cnn = cnn_model.fit(\n",
    "    X_train_final, y_train_final_cat,\n",
    "    validation_data=(X_val, y_val_cat),\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "cnn_time = time.time() - start_time\n",
    "print(f\"Training time: {cnn_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f1cccf-8f3d-4c01-b57f-c771e193e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CNN training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_cnn.history['loss'], label='Train Loss')\n",
    "plt.plot(history_cnn.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('CNN Training and Validation Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_cnn.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history_cnn.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('CNN Training and Validation Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Validation accuracy\n",
    "val_loss, val_acc = cnn_model.evaluate(X_val, y_val_cat, verbose=0)\n",
    "print(f\"CNN Validation Accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a57b2d3-ef86-4593-8f1a-ec26df8e5187",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Performance Summary:\n",
    "\n",
    "##Random Forest: 46.3% validation accuracy (quick baseline)\n",
    "##MLP: 40.5% validation accuracy (struggling, signs of overfitting)\n",
    "##CNN: 73.6% validation accuracy (much better!)\n",
    "\n",
    "##Key Observations:\n",
    "\n",
    "##MLP Issues: The MLP shows clear overfitting - training accuracy (~37%) is lower than validation (~40%), and the validation loss plateaus early. The gap suggests the model isn't learning spatial features well from flattened images.\n",
    "##CNN Success: The CNN performs significantly better because it preserves spatial structure. Notice how training and validation accuracy track closely, showing good generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4911b1",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10654e59-ef47-4cff-94c4-05798d24fe5b",
   "metadata": {},
   "source": [
    "### Algorithm of choice from first six weeks of course- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a75b98-38d0-4dfb-9ec9-b0b4fb3cbba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Hyperparameter Tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameter grid for Random Forest\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "print(\"Starting Random Forest hyperparameter search...\")\n",
    "print(f\"Total combinations: {len(rf_param_grid['n_estimators']) * len(rf_param_grid['max_depth']) * len(rf_param_grid['min_samples_split']) * len(rf_param_grid['min_samples_leaf'])}\")\n",
    "\n",
    "# Use a smaller subset for faster tuning (optional)\n",
    "# X_train_subset = X_train_flat[:10000]\n",
    "# y_train_subset = y_train_final[:10000]\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    rf_param_grid,\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "rf_grid.fit(X_train_flat, y_train_final)\n",
    "rf_search_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nSearch completed in {rf_search_time:.2f} seconds\")\n",
    "print(f\"Best parameters: {rf_grid.best_params_}\")\n",
    "print(f\"Best cross-validation score: {rf_grid.best_score_:.4f}\")\n",
    "\n",
    "# Store results for analysis\n",
    "rf_results = rf_grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e795c36f-cf3a-4d5f-8dcc-9e08bd646bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Random Forest hyperparameter search results\n",
    "import pandas as pd\n",
    "\n",
    "rf_results_df = pd.DataFrame(rf_results)\n",
    "\n",
    "# Plot effect of n_estimators\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# n_estimators vs accuracy\n",
    "for max_depth in [10, 20, 30, None]:\n",
    "    mask = rf_results_df['param_max_depth'] == max_depth\n",
    "    data = rf_results_df[mask].groupby('param_n_estimators')['mean_test_score'].mean()\n",
    "    axes[0, 0].plot(data.index, data.values, marker='o', label=f'max_depth={max_depth}')\n",
    "axes[0, 0].set_xlabel('n_estimators')\n",
    "axes[0, 0].set_ylabel('Mean CV Accuracy')\n",
    "axes[0, 0].set_title('Effect of n_estimators')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# max_depth vs accuracy\n",
    "depth_vals = [10, 20, 30, None]\n",
    "depth_scores = [rf_results_df[rf_results_df['param_max_depth'] == d]['mean_test_score'].mean() \n",
    "                for d in depth_vals]\n",
    "axes[0, 1].bar(range(len(depth_vals)), depth_scores)\n",
    "axes[0, 1].set_xticks(range(len(depth_vals)))\n",
    "axes[0, 1].set_xticklabels([str(d) for d in depth_vals])\n",
    "axes[0, 1].set_xlabel('max_depth')\n",
    "axes[0, 1].set_ylabel('Mean CV Accuracy')\n",
    "axes[0, 1].set_title('Effect of max_depth')\n",
    "axes[0, 1].grid(True, axis='y')\n",
    "\n",
    "# min_samples_split vs accuracy\n",
    "for n_est in [50, 100, 200]:\n",
    "    mask = rf_results_df['param_n_estimators'] == n_est\n",
    "    data = rf_results_df[mask].groupby('param_min_samples_split')['mean_test_score'].mean()\n",
    "    axes[1, 0].plot(data.index, data.values, marker='o', label=f'n_estimators={n_est}')\n",
    "axes[1, 0].set_xlabel('min_samples_split')\n",
    "axes[1, 0].set_ylabel('Mean CV Accuracy')\n",
    "axes[1, 0].set_title('Effect of min_samples_split')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Runtime analysis\n",
    "axes[1, 1].scatter(rf_results_df['mean_fit_time'], rf_results_df['mean_test_score'], alpha=0.6)\n",
    "axes[1, 1].set_xlabel('Mean Fit Time (seconds)')\n",
    "axes[1, 1].set_ylabel('Mean CV Accuracy')\n",
    "axes[1, 1].set_title('Accuracy vs Training Time Trade-off')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d0ab9b-4769-46a2-b0d6-542941bcd862",
   "metadata": {},
   "source": [
    "### Fully connected neural network- Multilayer Perceptron(MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada409dc-0d09-4afe-ac34-e149c7d9b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP Hyperparameter Tuning\n",
    "\n",
    "from keras_tuner import RandomSearch\n",
    "\n",
    "def build_mlp_tuner(hp):\n",
    "    \"\"\"Build MLP model with hyperparameters to tune\"\"\"\n",
    "    \n",
    "    # Hyperparameters to tune\n",
    "    n_layers = hp.Int('n_layers', min_value=1, max_value=3, step=1)\n",
    "    units_layer1 = hp.Choice('units_layer1', values=[128, 256, 512])\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)\n",
    "    learning_rate = hp.Choice('learning_rate', values=[0.001, 0.0001])\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(3072,)))\n",
    "    \n",
    "    # First layer\n",
    "    model.add(layers.Dense(units_layer1, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Additional layers\n",
    "    for i in range(n_layers - 1):\n",
    "        units = hp.Choice(f'units_layer{i+2}', values=[64, 128, 256])\n",
    "        model.add(layers.Dense(units, activation='relu'))\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Set up tuner\n",
    "mlp_tuner = RandomSearch(\n",
    "    build_mlp_tuner,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=20,  # Number of different configurations to try\n",
    "    executions_per_trial=1,\n",
    "    directory='mlp_tuning',\n",
    "    project_name='cifar10_mlp'\n",
    ")\n",
    "\n",
    "print(\"MLP Hyperparameter Search\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Search\n",
    "start_time = time.time()\n",
    "mlp_tuner.search(\n",
    "    X_train_flat, y_train_final_cat,\n",
    "    validation_data=(X_val_flat, y_val_cat),\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)],\n",
    "    verbose=0\n",
    ")\n",
    "mlp_search_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nSearch completed in {mlp_search_time:.2f} seconds\")\n",
    "print(f\"\\nBest hyperparameters: {mlp_tuner.get_best_hyperparameters()[0].values}\")\n",
    "\n",
    "# Get best model\n",
    "best_mlp = mlp_tuner.get_best_models(num_models=1)[0]\n",
    "val_loss, val_acc = best_mlp.evaluate(X_val_flat, y_val_cat, verbose=0)\n",
    "print(f\"Best model validation accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6caf37-a57c-47e0-9440-b083c8ed7bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MLP tuning results\n",
    "mlp_trials = []\n",
    "for trial in mlp_tuner.oracle.trials.values():\n",
    "    trial_data = {\n",
    "        'score': trial.score,\n",
    "        **trial.hyperparameters.values\n",
    "    }\n",
    "    mlp_trials.append(trial_data)\n",
    "\n",
    "mlp_trials_df = pd.DataFrame(mlp_trials)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Learning rate effect\n",
    "mlp_trials_df.groupby('learning_rate')['score'].mean().plot(kind='bar', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Effect of Learning Rate')\n",
    "axes[0, 0].set_ylabel('Validation Accuracy')\n",
    "axes[0, 0].grid(True, axis='y')\n",
    "\n",
    "# Dropout rate effect\n",
    "axes[0, 1].scatter(mlp_trials_df['dropout_rate'], mlp_trials_df['score'], alpha=0.6)\n",
    "axes[0, 1].set_xlabel('Dropout Rate')\n",
    "axes[0, 1].set_ylabel('Validation Accuracy')\n",
    "axes[0, 1].set_title('Effect of Dropout Rate')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Number of layers\n",
    "mlp_trials_df.groupby('n_layers')['score'].mean().plot(kind='bar', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Effect of Number of Layers')\n",
    "axes[1, 0].set_ylabel('Validation Accuracy')\n",
    "axes[1, 0].grid(True, axis='y')\n",
    "\n",
    "# Units in first layer\n",
    "mlp_trials_df.groupby('units_layer1')['score'].mean().plot(kind='bar', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Effect of Units in First Layer')\n",
    "axes[1, 1].set_ylabel('Validation Accuracy')\n",
    "axes[1, 1].grid(True, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11f6a3e-ec31-42f8-b310-13d925326aaf",
   "metadata": {},
   "source": [
    "### Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30adf541-f790-4214-b955-879dad492af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Hyperparameter Tuning\n",
    "\n",
    "def build_cnn_tuner(hp):\n",
    "    \"\"\"Build CNN model with hyperparameters to tune\"\"\"\n",
    "    \n",
    "    # Hyperparameters to tune\n",
    "    filters_1 = hp.Choice('filters_1', values=[32, 64])\n",
    "    filters_2 = hp.Choice('filters_2', values=[64, 128])\n",
    "    kernel_size = hp.Choice('kernel_size', values=[3, 5])\n",
    "    dense_units = hp.Choice('dense_units', values=[64, 128, 256])\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)\n",
    "    learning_rate = hp.Choice('learning_rate', values=[0.001, 0.0001])\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # First conv block\n",
    "    model.add(layers.Conv2D(filters_1, (kernel_size, kernel_size), \n",
    "                            activation='relu', padding='same', \n",
    "                            input_shape=(32, 32, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Second conv block\n",
    "    model.add(layers.Conv2D(filters_2, (kernel_size, kernel_size), \n",
    "                            activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(dense_units, activation='relu'))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Set up tuner\n",
    "cnn_tuner = RandomSearch(\n",
    "    build_cnn_tuner,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    directory='cnn_tuning',\n",
    "    project_name='cifar10_cnn'\n",
    ")\n",
    "\n",
    "print(\"CNN Hyperparameter Search\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Search\n",
    "start_time = time.time()\n",
    "cnn_tuner.search(\n",
    "    X_train_final, y_train_final_cat,\n",
    "    validation_data=(X_val, y_val_cat),\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)],\n",
    "    verbose=0\n",
    ")\n",
    "cnn_search_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nSearch completed in {cnn_search_time:.2f} seconds\")\n",
    "print(f\"\\nBest hyperparameters: {cnn_tuner.get_best_hyperparameters()[0].values}\")\n",
    "\n",
    "# Get best model\n",
    "best_cnn = cnn_tuner.get_best_models(num_models=1)[0]\n",
    "val_loss, val_acc = best_cnn.evaluate(X_val, y_val_cat, verbose=0)\n",
    "print(f\"Best model validation accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bcfed6-e4a3-43f8-ba9e-f72e411645c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CNN tuning results\n",
    "cnn_trials = []\n",
    "for trial in cnn_tuner.oracle.trials.values():\n",
    "    trial_data = {\n",
    "        'score': trial.score,\n",
    "        **trial.hyperparameters.values\n",
    "    }\n",
    "    cnn_trials.append(trial_data)\n",
    "\n",
    "cnn_trials_df = pd.DataFrame(cnn_trials)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# Filter configurations\n",
    "axes[0, 0].scatter(cnn_trials_df['filters_1'], cnn_trials_df['score'], alpha=0.6)\n",
    "axes[0, 0].set_xlabel('Filters in Layer 1')\n",
    "axes[0, 0].set_ylabel('Validation Accuracy')\n",
    "axes[0, 0].set_title('Effect of First Layer Filters')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "axes[0, 1].scatter(cnn_trials_df['filters_2'], cnn_trials_df['score'], alpha=0.6)\n",
    "axes[0, 1].set_xlabel('Filters in Layer 2')\n",
    "axes[0, 1].set_ylabel('Validation Accuracy')\n",
    "axes[0, 1].set_title('Effect of Second Layer Filters')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Kernel size\n",
    "cnn_trials_df.groupby('kernel_size')['score'].mean().plot(kind='bar', ax=axes[0, 2])\n",
    "axes[0, 2].set_title('Effect of Kernel Size')\n",
    "axes[0, 2].set_ylabel('Validation Accuracy')\n",
    "axes[0, 2].grid(True, axis='y')\n",
    "\n",
    "# Dense units\n",
    "cnn_trials_df.groupby('dense_units')['score'].mean().plot(kind='bar', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Effect of Dense Units')\n",
    "axes[1, 0].set_ylabel('Validation Accuracy')\n",
    "axes[1, 0].grid(True, axis='y')\n",
    "\n",
    "# Dropout\n",
    "axes[1, 1].scatter(cnn_trials_df['dropout_rate'], cnn_trials_df['score'], alpha=0.6)\n",
    "axes[1, 1].set_xlabel('Dropout Rate')\n",
    "axes[1, 1].set_ylabel('Validation Accuracy')\n",
    "axes[1, 1].set_title('Effect of Dropout Rate')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "# Learning rate\n",
    "cnn_trials_df.groupby('learning_rate')['score'].mean().plot(kind='bar', ax=axes[1, 2])\n",
    "axes[1, 2].set_title('Effect of Learning Rate')\n",
    "axes[1, 2].set_ylabel('Validation Accuracy')\n",
    "axes[1, 2].grid(True, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70e1068-e965-44ea-9997-a1473103c972",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Results:\n",
    "\n",
    "Random Forest: 45.89% CV accuracy\n",
    "\n",
    "Best: 200 trees, no max depth, min_samples_split=5\n",
    "Training time: ~37 minutes\n",
    "\n",
    "\n",
    "MLP: 50.11% validation accuracy\n",
    "\n",
    "Best: 2 layers (512→128 units), dropout=0.2, lr=0.0001\n",
    "Search time: ~5 minutes\n",
    "\n",
    "\n",
    "CNN: 73.55% validation accuracy\n",
    "\n",
    "Best: 64→128 filters, kernel=3x3, 256 dense units, dropout=0.3, lr=0.001\n",
    "Search time: ~76 minutes\n",
    "\n",
    "\n",
    "\n",
    "Key Insights from Visualizations:\n",
    "Random Forest:\n",
    "\n",
    "More trees = better performance (diminishing returns after 100)\n",
    "Unlimited depth works best (overfitting isn't an issue)\n",
    "Clear time-accuracy tradeoff visible\n",
    "\n",
    "MLP:\n",
    "\n",
    "Lower learning rate (0.0001) performs better\n",
    "Dropout around 0.2-0.3 is optimal\n",
    "2 layers with 512→128 units gives best results\n",
    "Still struggles compared to CNN (~50% vs ~74%)\n",
    "\n",
    "CNN:\n",
    "\n",
    "More filters (64, 128) perform better\n",
    "Kernel size 3 slightly better than 5\n",
    "Dropout 0.3 optimal\n",
    "Learning rate 0.001 works well\n",
    "Consistently strong performance (70%+)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eead5c82",
   "metadata": {},
   "source": [
    "## 4. Final models\n",
    "In this section, please ensure to include cells to train each model with its best hyperparmater combination independently of the hyperparameter tuning cells, i.e. don't rely on the hyperparameter tuning cells having been run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acace92d-cf5f-47c8-9fe7-1484fbf64e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FINAL MODEL TRAINING AND EVALUATION\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59f0415-677f-4aae-a448-b2e77878190a",
   "metadata": {},
   "source": [
    "### Algorithm of choice from first six weeks of course- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d104241-0e72-42e2-9d66-73d8d4c840a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Random Forest - Final Model\n",
    "print(\"\\n1. Random Forest - Training final model...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "rf_final = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "rf_final.fit(X_train_flat, y_train_final)\n",
    "rf_train_time = time.time() - start_time\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred_rf = rf_final.predict(X_test_flat)\n",
    "rf_test_accuracy = accuracy_score(y_test, y_test_pred_rf)\n",
    "\n",
    "print(f\"Training time: {rf_train_time:.2f} seconds\")\n",
    "print(f\"Test accuracy: {rf_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24812c78-643c-4c7b-9f88-95fbe8e4b456",
   "metadata": {},
   "source": [
    "### Fully connected neural network- Multilayer Perceptron(MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76e7a4c-2630-4e3d-a8fa-55236e0042be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. MLP - Final Model\n",
    "print(\"\\n2. MLP - Training final model...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "mlp_final = create_mlp(\n",
    "    input_shape=(3072,),\n",
    "    hidden_layers=[512, 128],\n",
    "    dropout_rate=0.2,\n",
    "    learning_rate=0.0001\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "start_time = time.time()\n",
    "history_mlp_final = mlp_final.fit(\n",
    "    X_train_flat, y_train_final_cat,\n",
    "    validation_data=(X_val_flat, y_val_cat),\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=0\n",
    ")\n",
    "mlp_train_time = time.time() - start_time\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, mlp_test_accuracy = mlp_final.evaluate(X_test_flat, to_categorical(y_test, 10), verbose=0)\n",
    "\n",
    "print(f\"Training time: {mlp_train_time:.2f} seconds\")\n",
    "print(f\"Epochs trained: {len(history_mlp_final.history['loss'])}\")\n",
    "print(f\"Test accuracy: {mlp_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4c3d53-d85a-438d-a02b-9d3a568d3a59",
   "metadata": {},
   "source": [
    "### Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394623ed-2b30-4fe9-b79e-ddba5bd7941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. CNN - Final Model\n",
    "print(\"\\n3. CNN - Training final model...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "cnn_final = create_cnn(\n",
    "    input_shape=(32, 32, 3),\n",
    "    filters=[64, 128],\n",
    "    kernel_size=3,\n",
    "    dense_units=256,\n",
    "    dropout_rate=0.3,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "history_cnn_final = cnn_final.fit(\n",
    "    X_train_final, y_train_final_cat,\n",
    "    validation_data=(X_val, y_val_cat),\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=0\n",
    ")\n",
    "cnn_train_time = time.time() - start_time\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, cnn_test_accuracy = cnn_final.evaluate(X_test_normalized, to_categorical(y_test, 10), verbose=0)\n",
    "\n",
    "print(f\"Training time: {cnn_train_time:.2f} seconds\")\n",
    "print(f\"Epochs trained: {len(history_cnn_final.history['loss'])}\")\n",
    "print(f\"Test accuracy: {cnn_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c934bd6e-6edd-4269-a835-4702be8b477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results_summary = pd.DataFrame({\n",
    "    'Algorithm': ['Random Forest', 'MLP', 'CNN'],\n",
    "    'Best Hyperparameters': [\n",
    "        'n_est=200, max_depth=None, min_split=5',\n",
    "        'layers=[512,128], dropout=0.2, lr=0.0001',\n",
    "        'filters=[64,128], k=3, dense=256, dropout=0.3, lr=0.001'\n",
    "    ],\n",
    "    'Training Time (s)': [rf_train_time, mlp_train_time, cnn_train_time],\n",
    "    'Test Accuracy': [rf_test_accuracy, mlp_test_accuracy, cnn_test_accuracy],\n",
    "    'Parameters': [\n",
    "        'N/A (ensemble)',\n",
    "        mlp_final.count_params(),\n",
    "        cnn_final.count_params()\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(results_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6325b3-e806-4883-8900-a851e21ea1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrices\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Random Forest\n",
    "cm_rf = confusion_matrix(y_test, y_test_pred_rf)\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', ax=axes[0], \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "axes[0].set_title(f'Random Forest\\nAccuracy: {rf_test_accuracy:.3f}')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# MLP\n",
    "y_test_pred_mlp = np.argmax(mlp_final.predict(X_test_flat), axis=1)\n",
    "cm_mlp = confusion_matrix(y_test, y_test_pred_mlp)\n",
    "sns.heatmap(cm_mlp, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "axes[1].set_title(f'MLP\\nAccuracy: {mlp_test_accuracy:.3f}')\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "\n",
    "# CNN\n",
    "y_test_pred_cnn = np.argmax(cnn_final.predict(X_test_normalized), axis=1)\n",
    "cm_cnn = confusion_matrix(y_test, y_test_pred_cnn)\n",
    "sns.heatmap(cm_cnn, annot=True, fmt='d', cmap='Blues', ax=axes[2],\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "axes[2].set_title(f'CNN\\nAccuracy: {cnn_test_accuracy:.3f}')\n",
    "axes[2].set_ylabel('True Label')\n",
    "axes[2].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e80559d-583d-4005-b472-c7fa88d8c200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class accuracy analysis\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PER-CLASS PERFORMANCE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, y_pred in [('Random Forest', y_test_pred_rf), \n",
    "                      ('MLP', y_test_pred_mlp), \n",
    "                      ('CNN', y_test_pred_cnn)]:\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ass2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
