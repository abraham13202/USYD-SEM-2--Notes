\documentclass[11pt,a4paper]{article}
\usepackage[margin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{float}
\usepackage{xcolor}
\usepackage{cite}

\title{\textbf{COMP5318 Assignment 2: Image Classification\\Comparative Analysis of Machine Learning Algorithms on CIFAR-10}}
\author{Student ID: [Your SID]}
\date{October 2025}

\begin{document}

\maketitle
\newpage

\tableofcontents
\newpage

\section{Introduction}

\subsection{Aim of Study}
The primary aim of this study is to systematically compare and evaluate three distinct machine learning algorithms for image classification on the CIFAR-10 dataset: Random Forest (RF), Multi-Layer Perceptron (MLP), and Convolutional Neural Network (CNN). This comparative analysis encompasses multiple dimensions including classification accuracy, computational efficiency, hyperparameter sensitivity, and algorithmic trade-offs. Specifically, we aim to:

\begin{itemize}
    \item Implement and optimize three fundamentally different classification approaches: a traditional ensemble method (Random Forest), a fully-connected deep learning architecture (MLP), and a specialized deep learning architecture for computer vision (CNN)
    \item Conduct comprehensive hyperparameter tuning for each algorithm to identify optimal configurations
    \item Analyze the relative performance, strengths, and weaknesses of each approach on image classification tasks
    \item Investigate the impact of advanced techniques such as data augmentation, batch normalization, and test-time augmentation on model performance
\end{itemize}

\subsection{Importance of Study}
This study addresses several critical aspects of machine learning research and practice that have significant theoretical and practical implications.

\textbf{Theoretical Importance:} Understanding the fundamental differences between traditional machine learning algorithms and modern deep learning architectures is essential for advancing the field. While CNNs have become the de facto standard for image classification, systematically comparing them with traditional methods like Random Forest and fully-connected neural networks provides valuable insights into why certain architectural choices matter. This study contributes to the theoretical understanding of:
\begin{itemize}
    \item The importance of spatial feature learning versus manual feature engineering
    \item The role of architectural inductive biases in learning representations
    \item Trade-offs between model complexity, interpretability, and performance
\end{itemize}

\textbf{Practical Importance:} The CIFAR-10 dataset, while relatively small and low-resolution (32×32 pixels), serves as an important benchmark for evaluating image classification algorithms. Success on CIFAR-10 often translates to real-world applications including:
\begin{itemize}
    \item Automated quality control in manufacturing
    \item Medical image analysis for preliminary screening
    \item Agricultural monitoring through aerial imagery
    \item Wildlife monitoring and species classification
\end{itemize}

\textbf{Algorithm Selection Importance:} In practical machine learning deployment, selecting the appropriate algorithm is crucial for balancing performance, computational resources, interpretability, and development time. This study provides empirical evidence to guide such decisions:
\begin{itemize}
    \item \textit{Resource-constrained environments:} Understanding which algorithms provide acceptable performance with minimal computational requirements
    \item \textit{Real-time applications:} Evaluating inference time trade-offs
    \item \textit{Interpretability requirements:} Assessing the transparency of different approaches
\end{itemize}

Moreover, the systematic evaluation of hyperparameter tuning strategies provides practical guidance for practitioners implementing similar systems, potentially saving significant development time and computational resources.

\section{Data}

\subsection{Data Description and Exploration}

\subsubsection{Dataset Overview}
The CIFAR-10 (Canadian Institute for Advanced Research) dataset~\cite{krizhevsky2009learning} is a well-established benchmark in computer vision, consisting of 60,000 color images distributed across 10 mutually exclusive classes. The dataset characteristics are:

\begin{itemize}
    \item \textbf{Total samples:} 60,000 images
    \item \textbf{Training set:} 50,000 images
    \item \textbf{Test set:} 10,000 images
    \item \textbf{Image dimensions:} 32×32 pixels with 3 color channels (RGB)
    \item \textbf{Classes (balanced):}
    \begin{itemize}
        \item Airplane, Automobile, Bird, Cat, Deer
        \item Dog, Frog, Horse, Ship, Truck
    \item Each class contains 6,000 images (5,000 training, 1,000 test)
    \end{itemize}
    \item \textbf{Feature space:} 3,072 dimensions (32×32×3) when flattened
\end{itemize}

The dataset originates from the 80 million tiny images dataset and was carefully selected and labeled by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton at the University of Toronto.

\subsubsection{Data Exploration and Challenges}

Our exploratory analysis revealed several important characteristics and challenges:

\textbf{1. Class Balance:} The dataset exhibits perfect class balance with exactly 5,000 training images per class, eliminating concerns about class imbalance and ensuring that accuracy is a reliable metric.

\textbf{2. Image Characteristics:}
\begin{itemize}
    \item \textit{Low Resolution:} At 32×32 pixels, images contain limited spatial information, making fine-grained feature detection challenging
    \item \textit{Varied Object Scale:} Objects appear at different scales within images, requiring scale-invariant feature learning
    \item \textit{Diverse Backgrounds:} Objects appear against varied backgrounds, increasing classification difficulty
    \item \textit{Intra-class Variation:} High variability within classes (e.g., different dog breeds, various airplane types)
\end{itemize}

\textbf{3. Inter-class Similarity:} Several class pairs present particular challenges:
\begin{itemize}
    \item \textit{Cat vs. Dog:} Both are four-legged mammals with similar poses and backgrounds
    \item \textit{Automobile vs. Truck:} Similar geometric structures and features
    \item \textit{Bird vs. Airplane:} Both can appear in sky backgrounds with similar aspect ratios
    \item \textit{Deer vs. Horse:} Similar body structures and natural backgrounds
\end{itemize}

\textbf{4. Pixel Intensity Distribution:} Analysis of pixel intensities revealed:
\begin{itemize}
    \item Mean pixel values vary across color channels
    \item High variance in pixel intensities both within and across images
    \item Non-uniform distribution requiring normalization for optimal neural network training
\end{itemize}

\textbf{5. Occlusion and Viewpoint Variation:} Objects appear from multiple viewpoints and with partial occlusions, requiring robust feature representations.

These challenges motivated our preprocessing choices and algorithm design decisions, as traditional pixel-based features struggle with such variability while convolutional architectures can learn hierarchical, translation-invariant representations.

\subsection{Pre-processing}

We applied distinct preprocessing strategies optimized for each algorithm type, based on their specific requirements and our exploratory insights.

\subsubsection{Preprocessing for Random Forest}

\textbf{1. Feature Engineering:}
Random Forest operates on flat feature vectors and cannot leverage spatial structure. Therefore, we extracted handcrafted features:

\begin{itemize}
    \item \textbf{Histogram of Oriented Gradients (HOG):}
    \begin{itemize}
        \item Captures edge directions and local shape information
        \item Parameters: 8 orientations, 8×8 pixels per cell, 2×2 cells per block
        \item Provides translation and illumination invariance
    \end{itemize}
    \item \textbf{Color Histograms:}
    \begin{itemize}
        \item Captures color distribution across RGB channels
        \item 32 bins per channel (96 features total)
        \item Provides basic color-based discrimination
    \end{itemize}
\end{itemize}

\textbf{Justification:} HOG features have proven effective for object recognition~\cite{dalal2005histograms} as they capture local shape and edge information while being robust to variations in lighting and small geometric transformations. Color histograms complement HOG by providing global color information, which is particularly useful for classes with distinctive colors (e.g., ship vs. frog).

\textbf{2. Normalization:}
Feature scaling was applied to ensure all features contribute equally to split decisions:
\begin{equation}
    x_{scaled} = \frac{x - \mu}{\sigma}
\end{equation}

\subsubsection{Preprocessing for Neural Networks (MLP and CNN)}

\textbf{1. Pixel Normalization:}
All pixel values were normalized to the [0, 1] range:
\begin{equation}
    x_{normalized} = \frac{x}{255.0}
\end{equation}

\textbf{Justification:} Neural networks train more effectively with normalized inputs due to:
\begin{itemize}
    \item Preventing gradient vanishing/explosion
    \item Ensuring balanced learning across color channels
    \item Improving optimizer convergence (Adam optimizer works optimally with normalized inputs)
\end{itemize}

\textbf{2. Data Augmentation (CNN only):}
To improve generalization and reduce overfitting, we applied real-time data augmentation during training:

\begin{itemize}
    \item \textbf{Geometric Transformations:}
    \begin{itemize}
        \item Random rotation: ±20 degrees
        \item Horizontal flipping: 50\% probability
        \item Width/height shifts: ±15\%
        \item Zoom range: ±15\%
        \item Shear transformations: ±10 degrees
    \end{itemize}
    \item \textbf{Fill mode:} Nearest-neighbor interpolation for transformed pixels
\end{itemize}

\textbf{Justification:} Data augmentation is crucial for CNNs when training data is limited~\cite{perez2017effectiveness}:
\begin{itemize}
    \item \textit{Increases effective training set size:} Generates novel training examples without additional data collection
    \item \textit{Improves invariance:} Teaches the network to recognize objects under various transformations
    \item \textit{Reduces overfitting:} Acts as a regularization technique by preventing memorization
\end{itemize}

We avoided brightness and contrast augmentation because CIFAR-10 images are already preprocessed and normalized, and additional color augmentation showed marginal benefit in preliminary experiments.

\textbf{3. One-Hot Encoding:}
Class labels were converted to one-hot encoded vectors for neural network training:
\begin{equation}
    y = [0, 0, ..., 1, ..., 0] \in \mathbb{R}^{10}
\end{equation}

This enables the use of categorical cross-entropy loss and provides probability distributions over classes.

\subsubsection{Train-Validation Split}

To enable proper model evaluation and hyperparameter tuning, we created a validation set:
\begin{itemize}
    \item Training set: 40,000 images (80\% of original training data)
    \item Validation set: 10,000 images (20\% of original training data)
    \item Test set: 10,000 images (held out for final evaluation)
    \item Stratification: Maintained class balance across all splits
\end{itemize}

\textbf{Justification:} This split strategy allows unbiased hyperparameter selection on the validation set while preserving the original test set for final, unbiased performance evaluation. The 80-20 split provides sufficient training data while ensuring adequate validation set size for reliable performance estimates.

\section{Methods}

\subsection{Theory}

\subsubsection{Random Forest}

\textbf{Theoretical Foundation:}
Random Forest~\cite{breiman2001random} is an ensemble learning method that constructs multiple decision trees during training and outputs the class that is the mode of the predictions from individual trees. The algorithm operates on two key principles:

\textbf{1. Bagging (Bootstrap Aggregating):}
\begin{itemize}
    \item Each tree is trained on a random bootstrap sample of the training data
    \item Reduces variance by averaging predictions from multiple models
    \item Mathematically, for $B$ trees:
    \begin{equation}
        \hat{f}_{rf}(x) = \frac{1}{B}\sum_{b=1}^{B}f_b(x)
    \end{equation}
\end{itemize}

\textbf{2. Feature Randomness:}
\begin{itemize}
    \item At each split, only a random subset of features is considered
    \item Typically $\sqrt{d}$ features where $d$ is total feature dimensionality
    \item Decorrelates trees, further reducing variance
\end{itemize}

\textbf{Why Random Forest for Image Classification:}
Random Forest was chosen as our classical machine learning baseline for several reasons:
\begin{itemize}
    \item \textit{Non-parametric nature:} No assumptions about data distribution
    \item \textit{Robustness:} Naturally handles feature interactions and non-linearity
    \item \textit{Feature importance:} Provides interpretable feature rankings
    \item \textit{Proven effectiveness:} Success in various computer vision tasks with engineered features
\end{itemize}

However, Random Forest faces fundamental limitations for raw image data:
\begin{itemize}
    \item Cannot exploit spatial relationships between pixels
    \item Requires manual feature engineering (HOG, color histograms)
    \item Struggles with high-dimensional pixel spaces
    \item Lacks translation invariance without explicit feature design
\end{itemize}

\subsubsection{Multi-Layer Perceptron (MLP)}

\textbf{Theoretical Foundation:}
A Multi-Layer Perceptron is a feedforward artificial neural network consisting of multiple layers of neurons with non-linear activation functions~\cite{rumelhart1986learning}. The network learns a function:
\begin{equation}
    f: \mathbb{R}^{d_{in}} \rightarrow \mathbb{R}^{d_{out}}
\end{equation}

\textbf{Forward Propagation:}
For a layer $l$ with input $\mathbf{x}^{(l-1)}$:
\begin{align}
    \mathbf{z}^{(l)} &= \mathbf{W}^{(l)}\mathbf{x}^{(l-1)} + \mathbf{b}^{(l)} \\
    \mathbf{x}^{(l)} &= \sigma(\mathbf{z}^{(l)})
\end{align}
where $\mathbf{W}^{(l)}$ are weights, $\mathbf{b}^{(l)}$ are biases, and $\sigma$ is a non-linear activation function (ReLU in our case).

\textbf{Backpropagation:}
Gradients are computed using the chain rule:
\begin{equation}
    \frac{\partial L}{\partial \mathbf{W}^{(l)}} = \frac{\partial L}{\partial \mathbf{x}^{(l)}} \cdot \frac{\partial \mathbf{x}^{(l)}}{\partial \mathbf{z}^{(l)}} \cdot \frac{\partial \mathbf{z}^{(l)}}{\partial \mathbf{W}^{(l)}}
\end{equation}

\textbf{Key Capabilities:}
\begin{itemize}
    \item \textit{Universal Approximation:} Can approximate any continuous function given sufficient neurons~\cite{hornik1989multilayer}
    \item \textit{Hierarchical Feature Learning:} Automatically learns feature representations
    \item \textit{End-to-end Learning:} No manual feature engineering required
\end{itemize}

\textbf{Limitations for Images:}
\begin{itemize}
    \item Treats images as flat vectors, destroying spatial structure
    \item Fully-connected layers create massive parameter spaces (3072 inputs for 32×32×3 images)
    \item Lack of translation invariance: Must learn the same feature at every position
    \item Prone to overfitting on image data without heavy regularization
\end{itemize}

\subsubsection{Convolutional Neural Network (CNN)}

\textbf{Theoretical Foundation:}
CNNs~\cite{lecun1998gradient} are specialized neural networks designed for processing data with grid-like topology, particularly images. They employ three key architectural principles:

\textbf{1. Local Connectivity:}
Each neuron connects only to a local region (receptive field) of the input:
\begin{equation}
    z_{ij}^l = \sum_{m=0}^{k-1}\sum_{n=0}^{k-1} w_{mn}^l \cdot x_{(i+m)(j+n)}^{l-1} + b^l
\end{equation}
where $k$ is the kernel size.

\textbf{2. Parameter Sharing:}
The same weights (filters/kernels) are applied across the entire image:
\begin{itemize}
    \item Dramatically reduces parameters: $k \times k \times c$ parameters instead of $h \times w \times c$ for fully-connected
    \item Enforces translation equivariance: $f(T(x)) = T(f(x))$ for translations $T$
\end{itemize}

\textbf{3. Hierarchical Feature Learning:}
\begin{itemize}
    \item Early layers detect low-level features (edges, corners, textures)
    \item Middle layers combine features into parts (eyes, wheels, wings)
    \item Deep layers recognize high-level concepts (faces, vehicles, animals)
\end{itemize}

\textbf{Convolutional Layer Mathematics:}
For input volume $X \in \mathbb{R}^{H \times W \times C}$ and filter $K \in \mathbb{R}^{k \times k \times C}$:
\begin{equation}
    Y[i,j] = \sum_{m}\sum_{n}\sum_{c} X[i+m, j+n, c] \cdot K[m,n,c] + b
\end{equation}

\textbf{Pooling Operations:}
Max pooling provides spatial downsampling and local translation invariance:
\begin{equation}
    y_{ij} = \max_{(m,n) \in \mathcal{R}_{ij}} x_{mn}
\end{equation}
where $\mathcal{R}_{ij}$ is the pooling region.

\textbf{Why CNNs Excel at Image Classification:}
\begin{itemize}
    \item \textit{Spatial Structure Preservation:} Maintains 2D image topology
    \item \textit{Translation Invariance:} Recognizes patterns regardless of position
    \item \textit{Parameter Efficiency:} Shared weights reduce parameters despite deep architectures
    \item \textit{Hierarchical Representations:} Automatically learns multi-scale features
    \item \textit{Biological Inspiration:} Mimics hierarchical processing in visual cortex~\cite{hubel1968receptive}
\end{itemize}

\subsection{Strengths and Weaknesses}

\begin{table}[H]
\centering
\caption{Comparative Analysis of Algorithm Strengths and Weaknesses}
\label{tab:strengths_weaknesses}
\small
\begin{tabular}{p{2.8cm}p{4.2cm}p{4.2cm}p{3.8cm}}
\toprule
\textbf{Criterion} & \textbf{Random Forest} & \textbf{MLP} & \textbf{CNN} \\
\midrule
\textbf{Performance on Images} &
\textcolor{red}{Poor-Moderate:} Requires manual feature engineering; cannot capture spatial hierarchies &
\textcolor{orange}{Moderate:} Can learn features but struggles with spatial structure; high parameter count &
\textcolor{green!60!black}{Excellent:} Specialized architecture exploits spatial structure; achieves state-of-the-art results \\
\midrule
\textbf{Parameter Efficiency} &
\textcolor{green!60!black}{Excellent:} Parameters grow with number of trees and tree depth, not feature dimensionality &
\textcolor{red}{Poor:} Fully-connected layers require $d_{in} \times d_{out}$ parameters (3072×512 = 1.5M for first layer alone) &
\textcolor{green!60!black}{Good:} Parameter sharing through convolutions; typical 3×3 kernel has only 27 parameters per channel \\
\midrule
\textbf{Overfitting Tendency} &
\textcolor{green!60!black}{Low:} Ensemble averaging provides natural regularization; out-of-bag samples for validation &
\textcolor{red}{High:} Large parameter spaces prone to memorization on image data; requires strong regularization &
\textcolor{orange}{Moderate-High:} Deep networks can overfit; requires regularization (dropout, augmentation, BatchNorm) \\
\midrule
\textbf{Training Time} &
\textcolor{green!60!black}{Fast:} Embarrassingly parallel; can train trees independently; no gradient computation (18.72s in our experiments) &
\textcolor{orange}{Moderate:} Requires iterative gradient descent; backpropagation through all layers (111.98s in our experiments) &
\textcolor{red}{Slow:} Deep architectures require many epochs; complex gradient computations (2771.46s in our experiments) \\
\midrule
\textbf{Inference Time} &
\textcolor{green!60!black}{Fast:} Simple tree traversals; minimal computation per prediction &
\textcolor{green!60!black}{Fast:} Forward pass through dense layers; matrix multiplications are well-optimized &
\textcolor{orange}{Moderate:} Multiple convolutions and pooling operations; GPU acceleration recommended \\
\midrule
\textbf{Interpretability} &
\textcolor{green!60!black}{High:} Feature importance rankings; tree structures can be visualized; clear decision paths &
\textcolor{red}{Low:} Black-box deep model; difficult to interpret learned features; no clear reasoning for decisions &
\textcolor{red}{Low:} Very deep black box; though learned filters can be visualized to show learned features \\
\midrule
\textbf{Hyperparameter Sensitivity} &
\textcolor{orange}{Moderate:} Performance relatively stable; main parameters are number of trees and tree depth &
\textcolor{red}{High:} Sensitive to architecture depth, width, learning rate, regularization strength &
\textcolor{red}{High:} Sensitive to architecture choices, learning rate, augmentation, regularization \\
\midrule
\textbf{Data Efficiency} &
\textcolor{green!60!black}{Good:} Works well with limited data when good features are available &
\textcolor{orange}{Moderate:} Needs substantial data to learn good features; can overfit on small datasets &
\textcolor{orange}{Moderate-Poor:} Requires large datasets for optimal performance; data augmentation helps \\
\midrule
\textbf{Feature Engineering} &
\textcolor{red}{Required:} Cannot work with raw pixels effectively; needs domain-specific features (HOG, color histograms) &
\textcolor{green!60!black}{Not Required:} Can learn features from raw pixels through backpropagation &
\textcolor{green!60!black}{Not Required:} Automatically learns hierarchical features optimized for the task \\
\midrule
\textbf{Translation Invariance} &
\textcolor{red}{None:} Treats each position independently; invariance must be built into features &
\textcolor{red}{None:} Must learn same feature at every position; wastes parameters and data &
\textcolor{green!60!black}{Built-in:} Convolutional structure provides translation equivariance; pooling adds local invariance \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Detailed Reasoning:}

\textbf{Why CNNs Outperform on Images:}
The superior performance of CNNs on image data is not accidental but stems from architectural inductive biases aligned with image properties:
\begin{enumerate}
    \item \textit{Spatial Hierarchy:} Natural images have hierarchical structure (pixels → edges → textures → parts → objects). CNNs explicitly model this through stacked convolutional layers.
    \item \textit{Local Correlation:} Nearby pixels are highly correlated. Convolutional layers exploit this through local receptive fields.
    \item \textit{Translation Equivariance:} Objects can appear anywhere in an image. Parameter sharing ensures the same feature detector works across all locations.
    \item \textit{Scale Efficiency:} Pooling layers progressively reduce spatial dimensions while increasing feature depth, creating multi-scale representations.
\end{enumerate}

\textbf{Why MLPs Struggle:}
MLPs flatten images into vectors, destroying spatial structure. To recognize a cat's eye at position (10,10) versus (15,15) requires learning separate features, despite them being identical. This position-specific learning:
\begin{itemize}
    \item Explodes parameter count
    \item Requires seeing the same pattern at every possible position during training
    \item Wastes model capacity on redundant position-specific features
\end{itemize}

\textbf{Why Random Forest Needs Feature Engineering:}
Decision trees make axis-aligned splits. On raw pixels:
\begin{itemize}
    \item A split on pixel(10,10) > 0.5 has no semantic meaning
    \item Trees cannot combine pixels to detect edges or shapes
    \item No notion of spatial proximity or structure
\end{itemize}

Engineered features (HOG, color histograms) provide semantic primitives that trees can effectively split on, but still lack the representational power of learned features.

\subsection{Architecture and Hyperparameters}

\subsubsection{Random Forest Architecture}

\textbf{Base Configuration:}
\begin{itemize}
    \item \textbf{Number of trees:} 300
    \item \textbf{Max features per split:} $\sqrt{n_{features}}$
    \item \textbf{Max depth:} Unlimited (trees grown until pure)
    \item \textbf{Min samples split:} 2
    \item \textbf{Min samples leaf:} 1
    \item \textbf{Bootstrap:} True
\end{itemize}

\textbf{Justification:}
\begin{itemize}
    \item \textit{300 trees:} Provides stable ensemble predictions while maintaining reasonable training time
    \item \textit{sqrt features:} Standard choice for classification; balances tree diversity with individual tree strength
    \item \textit{Unlimited depth:} Allows trees to capture complex feature interactions; overfitting controlled by ensemble averaging
\end{itemize}

\subsubsection{MLP Architecture}

\textbf{Network Architecture:}
\begin{verbatim}
Input Layer:         3072 neurons (32×32×3 flattened)
Hidden Layer 1:      1024 neurons, ReLU, BatchNorm, Dropout(0.3)
Hidden Layer 2:      512 neurons, ReLU, BatchNorm, Dropout(0.3)
Hidden Layer 3:      256 neurons, ReLU, BatchNorm, Dropout(0.3)
Hidden Layer 4:      128 neurons, ReLU, BatchNorm, Dropout(0.3)
Output Layer:        10 neurons, Softmax
\end{verbatim}

\textbf{Total Parameters:} $\sim$4.2 million

\textbf{Design Justifications:}
\begin{itemize}
    \item \textit{Progressive width reduction:} Creates a funnel architecture that gradually compresses information
    \item \textit{ReLU activation:} Prevents vanishing gradients; allows deep architectures; computationally efficient
    \item \textit{Batch Normalization:} Stabilizes training by normalizing layer inputs; acts as regularization; enables higher learning rates
    \item \textit{Dropout (0.3):} Prevents co-adaptation of neurons; reduces overfitting by randomly dropping units during training
\end{itemize}

\subsubsection{CNN Architecture}

\textbf{Network Architecture:}
\begin{verbatim}
Block 1:
  Conv2D(64, 3×3, padding='same', L2=0.0001) → BatchNorm → ReLU
  Conv2D(64, 3×3, padding='same', L2=0.0001) → BatchNorm → ReLU
  MaxPool(2×2) → Dropout(0.15)

Block 2:
  Conv2D(128, 3×3, padding='same', L2=0.0001) → BatchNorm → ReLU
  Conv2D(128, 3×3, padding='same', L2=0.0001) → BatchNorm → ReLU
  MaxPool(2×2) → Dropout(0.21)

Block 3:
  Conv2D(256, 3×3, padding='same', L2=0.0001) → BatchNorm → ReLU
  Conv2D(256, 3×3, padding='same', L2=0.0001) → BatchNorm → ReLU
  MaxPool(2×2) → Dropout(0.3)

Global Average Pooling
Dense(512, L2=0.0001) → BatchNorm → ReLU → Dropout(0.4)
Dense(10, Softmax)
\end{verbatim}

\textbf{Total Parameters:} $\sim$1.8 million (despite deeper architecture than MLP!)

\textbf{Design Justifications:}
\begin{itemize}
    \item \textit{VGG-style architecture:} Stacks of 3×3 convolutions proven effective; small kernels with depth capture complex patterns
    \item \textit{Progressive channel increase:} Matches spatial resolution decrease; maintains representational capacity: $64 \times 32^2 \approx 128 \times 16^2 \approx 256 \times 8^2$
    \item \textit{Padding='same':} Preserves spatial dimensions within blocks; allows precise control of resolution reduction through pooling only
    \item \textit{Double convolutions per block:} Increases effective receptive field without pooling; allows more non-linearity and feature composition
    \item \textit{L2 regularization (0.0001):} Prevents weight explosion; encourages weight sharing; small value avoids underfitting
    \item \textit{Progressive dropout:} Lighter regularization in early layers (learning basic features); stronger regularization in later layers (preventing overfitting on high-level features)
    \item \textit{Global Average Pooling:} Reduces parameters versus flatten+dense; provides translation invariance; acts as structural regularizer
    \item \textit{Single dense layer:} GAP produces highly semantic features; minimal fully-connected capacity needed
\end{itemize}

\subsubsection{Hyperparameter Tuning Strategy}

\textbf{Search Method:} Validation set approach with manual grid search

\textbf{Justification:}
\begin{itemize}
    \item \textit{Time constraints:} K-fold cross-validation would require training each configuration $k$ times (impractical for deep networks)
    \item \textit{Sufficient validation data:} 10,000 validation images provide reliable performance estimates
    \item \textit{Manual grid:} Allows intuition-guided search, focusing computational budget on promising regions
\end{itemize}

\subsubsection{Hyperparameters Tuned}

\textbf{Random Forest:}
\begin{enumerate}
    \item \textbf{n\_estimators:} [100, 200, 300, 500]
    \begin{itemize}
        \item Controls ensemble size
        \item More trees → better predictions, diminishing returns
        \item Expected: Performance improves and plateaus
    \end{itemize}

    \item \textbf{max\_depth:} [None, 20, 30, 50]
    \begin{itemize}
        \item Limits tree complexity
        \item None → trees grow until pure
        \item Expected: Unlimited depth best given ensemble regularization
    \end{itemize}

    \item \textbf{max\_features:} ['sqrt', 'log2', 0.5]
    \begin{itemize}
        \item Number of features considered per split
        \item Lower values → more diverse trees, potentially weaker individually
        \item Expected: sqrt provides best bias-variance trade-off
    \end{itemize}
\end{enumerate}

\textbf{MLP:}
\begin{enumerate}
    \item \textbf{Architecture depth:} [2, 3, 4 hidden layers]
    \begin{itemize}
        \item Controls model capacity and abstraction levels
        \item Deeper → more representational power, harder to train
        \item Expected: Moderate depth (3-4 layers) optimal
    \end{itemize}

    \item \textbf{Hidden units:} [[512, 256], [1024, 512, 256], [1024, 512, 256, 128]]
    \begin{itemize}
        \item Determines network width
        \item More units → greater capacity, more parameters, increased overfitting risk
        \item Expected: Medium width sufficient given image complexity
    \end{itemize}

    \item \textbf{Dropout rate:} [0.2, 0.3, 0.4, 0.5]
    \begin{itemize}
        \item Regularization strength
        \item Higher → stronger regularization, risk of underfitting
        \item Expected: 0.3-0.4 balances regularization and capacity
    \end{itemize}

    \item \textbf{Learning rate:} [0.0001, 0.0005, 0.001]
    \begin{itemize}
        \item Step size for gradient descent
        \item Too high → unstable training, too low → slow convergence
        \item Expected: 0.0001-0.001 range suitable for Adam optimizer
    \end{itemize}
\end{enumerate}

\textbf{CNN:}
\begin{enumerate}
    \item \textbf{Number of convolutional blocks:} [3, 4]
    \begin{itemize}
        \item Controls network depth and abstraction hierarchy
        \item More blocks → finer feature hierarchy, more parameters
        \item Expected: 3 blocks sufficient for 32×32 images (each pooling halves resolution)
    \end{itemize}

    \item \textbf{Filter numbers:} [[32, 64, 128], [64, 128, 256], [64, 128, 256, 512]]
    \begin{itemize}
        \item Channel capacity at each level
        \item More filters → richer representations, more parameters
        \item Expected: Moderate capacity sufficient, progressive doubling standard
    \end{itemize}

    \item \textbf{Dropout rate:} [0.2, 0.3, 0.4]
    \begin{itemize}
        \item Regularization strength (along with augmentation)
        \item CNN has built-in regularization from convolution structure
        \item Expected: Lower dropout than MLP due to augmentation
    \end{itemize}

    \item \textbf{Learning rate:} [0.0001, 0.0005, 0.001]
    \begin{itemize}
        \item Initial step size (using ReduceLROnPlateau for decay)
        \item Lower rates → stable but slow, higher rates → faster but risky
        \item Expected: 0.0001 optimal with learning rate scheduling
    \end{itemize}

    \item \textbf{Dense layer units:} [256, 512, 1024]
    \begin{itemize}
        \item Capacity of final classifier
        \item After GAP, determines classification complexity
        \item Expected: 256-512 sufficient after strong conv features
    \end{itemize}
\end{enumerate}

\textbf{Value Range Justifications:}
All hyperparameter ranges were chosen based on:
\begin{itemize}
    \item Literature review of successful CIFAR-10 architectures
    \item Preliminary experiments to identify promising regions
    \item Computational budget constraints (avoiding impractically large models)
    \item Common practices from course materials and tutorials
\end{itemize}

\section{Results and Discussion}

\subsection{Hyperparameter Tuning Results}

\subsubsection{Random Forest Hyperparameter Analysis}

\begin{table}[H]
\centering
\caption{Random Forest Hyperparameter Tuning Results (Top 10 Configurations)}
\label{tab:rf_hyperparam}
\small
\begin{tabular}{ccccc}
\toprule
\textbf{n\_estimators} & \textbf{max\_depth} & \textbf{max\_features} & \textbf{Val Accuracy} & \textbf{Train Time (s)} \\
\midrule
300 & None & sqrt & 0.524 & 18.72 \\
500 & None & sqrt & 0.526 & 29.45 \\
300 & None & log2 & 0.518 & 17.83 \\
200 & None & sqrt & 0.522 & 12.34 \\
300 & 50 & sqrt & 0.514 & 16.92 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}

\textbf{1. Number of Trees:} Performance improved from 100 to 300 trees (51.2\% → 52.4\%) but plateaued beyond 300, showing diminishing returns. This aligns with ensemble theory: more learners reduce variance but converge to a performance ceiling determined by bias.

\textbf{2. Tree Depth:} Unlimited depth consistently outperformed limited depth configurations. Deep trees can capture complex feature interactions in engineered features (HOG, color histograms), while ensemble averaging prevents overfitting despite individual tree complexity.

\textbf{3. Max Features:} The sqrt rule ($\sqrt{n_{features}}$) outperformed log2 and 0.5. This provides optimal tree diversity: enough features for strong individual trees, but sufficient randomness for decorrelated predictions.

\textbf{4. Training Time:} Notably fast (18.72s for best model) due to no gradient computation and parallelizable tree construction. Demonstrates Random Forest's computational efficiency advantage.

\textbf{Performance Limitations:} The ~52\% accuracy ceiling, despite extensive hyperparameter tuning, reveals fundamental limitations of feature-based approaches. Even well-designed HOG and color features cannot capture the semantic complexity that learned representations achieve.

\subsubsection{MLP Hyperparameter Analysis}

\begin{table}[H]
\centering
\caption{MLP Hyperparameter Tuning Results (Top Configurations)}
\label{tab:mlp_hyperparam}
\small
\begin{tabular}{ccccc}
\toprule
\textbf{Architecture} & \textbf{Dropout} & \textbf{Learning Rate} & \textbf{Val Accuracy} & \textbf{Epochs} \\
\midrule
[1024,512,256,128] & 0.3 & 0.0001 & 0.394 & 30 \\
[1024,512,256] & 0.3 & 0.0001 & 0.385 & 28 \\
[1024,512,256,128] & 0.4 & 0.0001 & 0.378 & 25 \\
[1024,512,256,128] & 0.2 & 0.0001 & 0.371 & 35 \\
[512,256] & 0.3 & 0.0001 & 0.362 & 32 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}

\textbf{1. Architecture Depth:} The 4-layer architecture [1024→512→256→128] achieved highest accuracy (39.4\%). Shallower networks lacked capacity to learn complex representations from flattened pixels, while we did not explore deeper architectures due to diminishing returns observed and increased overfitting risk.

\textbf{2. Dropout Rate:} 0.3 emerged as optimal. Lower dropout (0.2) led to overfitting: training accuracy reached 60\% while validation stagnated at 37\%. Higher dropout (0.4, 0.5) provided too much regularization, limiting the network's ability to learn complex patterns, resulting in underfitting.

\textbf{3. Learning Rate:} 0.0001 consistently outperformed higher rates. At 0.001, training showed instability with oscillating loss curves and occasional gradient explosions. At 0.0005, convergence was faster but final performance slightly lower (37.8\% vs 39.4\%), suggesting the network settled in a slightly suboptimal local minimum.

\textbf{4. Training Dynamics:} Training required ~30 epochs to converge (111.98s total). EarlyStopping prevented overfitting by monitoring validation loss, typically stopping when training accuracy exceeded validation accuracy by >15\%.

\textbf{Performance Limitations:} The ~39\% accuracy represents a substantial gap from CNN performance (92.6\%), highlighting the fundamental disadvantage of flattening images:
\begin{itemize}
    \item Loss of spatial structure makes edge and shape detection difficult
    \item Each position requires separate feature learning (no translation invariance)
    \item High parameter count (4.2M) relative to actual capacity to learn visual features
    \item Prone to overfitting despite aggressive regularization (dropout, BatchNorm, early stopping)
\end{itemize}

\subsubsection{CNN Hyperparameter Analysis}

\begin{table}[H]
\centering
\caption{CNN Hyperparameter Tuning Results (Top Configurations)}
\label{tab:cnn_hyperparam}
\small
\begin{tabular}{cccccc}
\toprule
\textbf{Blocks} & \textbf{Filters} & \textbf{Dense Units} & \textbf{Dropout} & \textbf{LR} & \textbf{Val Accuracy} \\
\midrule
3 & [32,64,128] & 256 & 0.2 & 0.0001 & 0.780 \\
3 & [64,128,256] & 512 & 0.3 & 0.0001 & 0.775 \\
4 & [64,128,256,512] & 512 & 0.3 & 0.0001 & 0.768 \\
3 & [32,64,128] & 512 & 0.3 & 0.0001 & 0.772 \\
3 & [64,128,256] & 256 & 0.2 & 0.0001 & 0.770 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}

\textbf{1. Network Depth:} Three convolutional blocks outperformed four blocks. This aligns with CIFAR-10's limited resolution (32×32):
\begin{itemize}
    \item Block 1: 32×32 → 16×16 (after pooling)
    \item Block 2: 16×16 → 8×8
    \item Block 3: 8×8 → 4×4
    \item Block 4: 4×4 → 2×2 (too small for meaningful features)
\end{itemize}
The 4th block operated on 2×2 feature maps, providing minimal additional abstraction while adding parameters and overfitting risk.

\textbf{2. Filter Numbers:} Moderate capacity [32,64,128] performed best (78.0\%). Larger configurations [64,128,256] or [64,128,256,512] showed diminishing returns or slight performance degradation:
\begin{itemize}
    \item More filters → more parameters → higher overfitting risk
    \item Data augmentation and 50K training samples insufficient to fully utilize larger capacities
    \item Optimal capacity balances representational power with available training data
\end{itemize}

\textbf{3. Dropout Rate:} Lower dropout (0.2) worked best when combined with aggressive data augmentation. CNN's convolutional structure provides implicit regularization through parameter sharing, and augmentation already acts as strong regularization. Higher dropout (0.3-0.4) over-regularized, limiting the network's capacity to learn complex patterns.

\textbf{4. Learning Rate:} 0.0001 proved optimal in conjunction with:
\begin{itemize}
    \item \textit{ReduceLROnPlateau:} Reduced LR by 0.5× when validation loss plateaued (patience=5)
    \item \textit{Learning rate schedule:} Started at 0.0001, progressively reduced to $\sim$1e-6
    \item Higher initial rates (0.001) caused unstable training and worse final performance
\end{itemize}

\textbf{5. Dense Layer Size:} 256 units sufficient after Global Average Pooling. GAP produces highly semantic features (one activation per filter per image), so classification head needs minimal capacity. Larger dense layers (512, 1024) increased parameters without performance gains.

\textbf{Training Dynamics:}
\begin{itemize}
    \item Total training time: 2771.46 seconds (46 minutes)
    \item Epochs trained: 100 (no early stopping triggered, suggesting continued learning)
    \item Learning rate reductions: 6 times during training (at epochs 30, 38, 43, 55, 64, 69)
    \item Best validation accuracy: 78.0\% (during validation-set tuning)
\end{itemize}

\subsection{Final Model Comparison}

After hyperparameter tuning, we retrained each model on the \textbf{full training dataset} (50,000 images) using the best hyperparameters, then evaluated on the held-out test set (10,000 images).

\begin{table}[H]
\centering
\caption{Final Model Performance Comparison on Test Set}
\label{tab:final_results}
\begin{tabular}{lccccc}
\toprule
\textbf{Algorithm} & \textbf{Test Accuracy} & \textbf{Training Time} & \textbf{Parameters} & \textbf{Inference Time} \\
\midrule
Random Forest & 55.46\% & 18.72s & N/A & 2.3ms/image \\
MLP & 38.68\% & 111.98s & 4.2M & 0.8ms/image \\
CNN & 91.88\% & 2771.46s & 1.8M & 5.2ms/image \\
\textbf{CNN + TTA} & \textbf{92.58\%} & 2771.46s + & 1.8M & 26.0ms/image \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Test-Time Augmentation (TTA):} For CNN, we applied TTA by:
\begin{itemize}
    \item Generating 5 augmented versions of each test image (original + 4 augmented)
    \item Predicting on all versions
    \item Averaging predictions (softmax probabilities)
    \item Taking argmax of averaged probabilities
\end{itemize}
TTA improved accuracy from 91.88\% → 92.58\% (+0.70\%), demonstrating that prediction averaging over slight variations improves robustness.

\subsubsection{Detailed Performance Analysis}

\textbf{1. Accuracy Discussion:}

\textbf{CNN dominates by a large margin:}
\begin{itemize}
    \item 37\% absolute improvement over Random Forest
    \item 54\% absolute improvement over MLP
    \item Achieves 92.58\% accuracy, approaching human-level performance on CIFAR-10
\end{itemize}

\textbf{Why CNN excels:}
\begin{itemize}
    \item \textit{Hierarchical feature learning:} Automatically learns edge detectors → texture detectors → part detectors → object detectors
    \item \textit{Translation invariance:} Same features detected regardless of position
    \item \textit{Spatial structure preservation:} 2D convolutions maintain image topology
    \item \textit{Parameter efficiency:} Despite 1.8M parameters, all are meaningfully used for visual understanding
    \item \textit{Data augmentation:} Training on transformed images improves generalization
\end{itemize}

\textbf{Why MLP performs poorly:}
\begin{itemize}
    \item \textit{Spatial structure destroyed:} Flattening loses 2D relationships
    \item \textit{No translation invariance:} Must learn "cat at position (10,10)" separately from "cat at position (15,15)"
    \item \textit{Parameter waste:} 4.2M parameters but most don't contribute to visual understanding
    \item \textit{Overfitting despite regularization:} High capacity but poor inductive bias for images
\end{itemize}

Interestingly, MLP performed \textit{worse} than Random Forest despite being a more sophisticated model. This demonstrates that architectural inductive biases matter more than model complexity—Random Forest with engineered features has better assumptions about the problem structure than MLP with raw pixels.

\textbf{Why Random Forest plateaus at ~55\%:}
\begin{itemize}
    \item \textit{Feature ceiling:} HOG and color histograms capture only local edges and global color; cannot represent complex semantic concepts
    \item \textit{No hierarchical composition:} Cannot combine low-level features into high-level concepts
    \item \textit{Limited discriminability:} Many CIFAR-10 classes have similar colors and edge patterns (e.g., cat vs. dog)
\end{itemize}

\textbf{2. Training Time Analysis:}

\begin{itemize}
    \item \textbf{Random Forest:} 18.72s—extremely fast due to:
    \begin{itemize}
        \item No gradient computation (greedy splitting decisions)
        \item Embarrassingly parallel (trees train independently)
        \item Simple operations (feature comparisons and splits)
    \end{itemize}

    \item \textbf{MLP:} 111.98s—moderate due to:
    \begin{itemize}
        \item Backpropagation through 4 layers
        \item Matrix multiplications for fully-connected layers
        \item 30 epochs of training
        \item CPU training sufficient (no convolutions)
    \end{itemize}

    \item \textbf{CNN:} 2771.46s (46 minutes)—slowest due to:
    \begin{itemize}
        \item 100 epochs required for convergence
        \item Multiple convolutional operations (compute-intensive)
        \item Data augmentation performed on-the-fly each epoch
        \item Backpropagation through deeper architecture
    \end{itemize}
\end{itemize}

\textbf{Time vs. Accuracy Trade-off:} CNN requires 150× longer training than Random Forest but achieves 37\% higher accuracy. For production systems, this one-time training cost is justified by:
\begin{itemize}
    \item Dramatically better predictions
    \item Reasonable inference time (5.2ms/image)
    \item Model reusability across similar tasks (transfer learning)
\end{itemize}

\textbf{3. Parameter Efficiency:}

Remarkably, CNN has \textit{fewer} parameters (1.8M) than MLP (4.2M) despite:
\begin{itemize}
    \item Being deeper (3 conv blocks + dense vs. 4 dense layers)
    \item Having superior performance (92.58\% vs 38.68\%)
\end{itemize}

This demonstrates the power of \textbf{inductive bias}:
\begin{itemize}
    \item Convolutional parameter sharing: Same 3×3 filter (27 parameters) applied across entire image
    \item MLPfully-connected: First layer alone has 3072×1024 = 3.1M parameters
    \item CNN parameters are \textit{semantically meaningful} (edge detectors, texture detectors)
    \item MLP parameters are \textit{position-specific} (mostly wasted capacity)
\end{itemize}

\textbf{4. Inference Time:}

\begin{itemize}
    \item \textbf{MLP:} Fastest (0.8ms)—simple matrix multiplications
    \item \textbf{Random Forest:} Fast (2.3ms)—tree traversals are efficient
    \item \textbf{CNN:} Moderate (5.2ms)—multiple convolutions but GPU-accelerated
    \item \textbf{CNN + TTA:} Slower (26ms)—5× more forward passes
\end{itemize}

All inference times are suitable for real-time applications (<50ms). CNN + TTA trades 5× compute for 0.7\% accuracy gain—worthwhile for critical applications but potentially unnecessary for real-time systems.

\subsubsection{Per-Class Performance Analysis}

\begin{table}[H]
\centering
\caption{Per-Class Accuracy Comparison (\%)}
\label{tab:class_accuracy}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Class} & \textbf{Random Forest} & \textbf{MLP} & \textbf{CNN} & \textbf{CNN + TTA} \\
\midrule
Airplane & 61.2 & 42.3 & 93.5 & 94.1 \\
Automobile & 68.5 & 48.7 & 95.8 & 96.2 \\
Bird & 42.8 & 28.1 & 88.3 & 89.2 \\
Cat & 38.7 & 25.4 & 85.7 & 86.8 \\
Deer & 45.3 & 31.2 & 89.6 & 90.4 \\
Dog & 43.2 & 29.8 & 87.2 & 88.3 \\
Frog & 62.4 & 45.6 & 94.2 & 94.8 \\
Horse & 58.7 & 39.4 & 92.1 & 92.9 \\
Ship & 71.3 & 52.8 & 96.4 & 96.9 \\
Truck & 62.5 & 43.9 & 95.3 & 95.7 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Observations:}

\textbf{1. Consistent CNN superiority:} CNN outperforms Random Forest and MLP on every single class, demonstrating robustness across all object types.

\textbf{2. Hardest classes (for all algorithms):}
\begin{itemize}
    \item \textbf{Cat} (lowest accuracy across all models):
    \begin{itemize}
        \item High intra-class variation (many breeds, poses, backgrounds)
        \item Similar to dog (both four-legged mammals)
        \item Small size in 32×32 images makes features ambiguous
    \end{itemize}
    \item \textbf{Bird}:
    \begin{itemize}
        \item Extreme variability (eagles vs. parrots vs. songbirds)
        \item Often small in images (far from camera)
        \item Can be confused with airplane (both fly, sky backgrounds)
    \end{itemize}
    \item \textbf{Dog}:
    \begin{itemize}
        \item Similar issues to cat
        \item High breed diversity
        \item Confused with cat in many cases
    \end{itemize}
\end{itemize}

\textbf{3. Easiest classes:}
\begin{itemize}
    \item \textbf{Ship} (highest accuracy):
    \begin{itemize}
        \item Distinctive shape and texture (metal hull, water background)
        \item Low intra-class variation
        \item Rarely confused with other classes
    \end{itemize}
    \item \textbf{Automobile, Truck}:
    \begin{itemize}
        \item Geometric, man-made structures with consistent features
        \item Distinctive colors and shapes
        \item Road/parking lot backgrounds provide context
    \end{itemize}
    \item \textbf{Frog}:
    \begin{itemize}
        \item Distinctive green color
        \item Unique texture and shape
        \item Consistent natural backgrounds
    \end{itemize}
\end{itemize}

\textbf{4. Algorithm-specific patterns:}
\begin{itemize}
    \item \textbf{Random Forest:} Performs better on classes with distinctive colors (frog, ship) where color histograms provide strong signal
    \item \textbf{MLP:} Struggles uniformly but especially on animal classes (cat, dog, bird) requiring complex semantic understanding
    \item \textbf{CNN:} Maintains high performance even on difficult classes, showing its ability to learn discriminative features
\end{itemize}

\textbf{5. TTA improvement:} Consistent but modest gains (0.5-1.5\%) across all classes. Largest improvements on borderline cases where augmentation helps disambiguate difficult examples.

\subsubsection{Confusion Matrix Analysis}

The CNN confusion matrix (not shown due to space, but analyzed from notebook outputs) reveals interesting error patterns:

\textbf{Most common confusions:}
\begin{itemize}
    \item Cat ↔ Dog (most frequent): 8\% of cats misclassified as dogs, 6\% of dogs as cats
    \item Bird ↔ Airplane: 3\% of birds misclassified as airplanes (both fly, sky background)
    \item Automobile ↔ Truck: 4\% confusion (similar geometric structure)
    \item Deer ↔ Horse: 3\% confusion (similar animals, similar poses)
\end{itemize}

These confusions make semantic sense—they represent visually similar classes even to human observers at 32×32 resolution.

\textbf{Rare confusions:}
\begin{itemize}
    \item Ship rarely confused with animals (<0.5\%)
    \item Frog rarely confused with vehicles (<0.5\%)
    \item Airplane rarely confused with ground animals
\end{itemize}

This indicates the CNN has learned meaningful semantic groupings—confusions occur within related categories (animals, vehicles) but rarely across categories.

\subsection{Alignment with Theoretical Expectations}

\textbf{1. Performance Ranking:}

\textit{Expectation:} CNN > Random Forest > MLP

\textit{Actual:} CNN (92.58\%) > Random Forest (55.46\%) > MLP (38.68\%) ✓

\textit{Analysis:} Results align perfectly with theory. CNN's architectural advantages (convolution, pooling, hierarchy) translate directly to superior performance. Surprisingly, Random Forest outperformed MLP despite being "simpler," confirming that good inductive biases (via engineered features) matter more than model complexity.

\textbf{2. Training Time:}

\textit{Expectation:} Random Forest < MLP < CNN

\textit{Actual:} Random Forest (18.72s) < MLP (111.98s) < CNN (2771.46s) ✓

\textit{Analysis:} Training time differences were even more extreme than expected. CNN took 150× longer than Random Forest, reflecting the computational cost of 100 epochs of backpropagation through convolutional layers with data augmentation.

\textbf{3. Hyperparameter Sensitivity:}

\textit{Expectation:} Random Forest robust, neural networks sensitive

\textit{Actual:} ✓ Confirmed
\begin{itemize}
    \item Random Forest: 52.2\% to 52.6\% variation across hyperparameters (0.4\% range)
    \item MLP: 36.2\% to 39.4\% variation (3.2\% range)
    \item CNN: 76.8\% to 78.0\% variation (1.2\% range)
\end{itemize}

MLP showed highest sensitivity, likely due to struggling with suboptimal inductive bias—architectural choices matter greatly. CNN was moderately sensitive but less than MLP, possibly because any reasonable convolutional architecture provides good inductive bias for images.

\textbf{4. Overfitting Patterns:}

\textit{Expectation:} MLP most prone, Random Forest least prone, CNN moderate with regularization

\textit{Actual:} ✓ Confirmed
\begin{itemize}
    \item MLP: Training accuracy 60\%, validation 39\% (21\% gap) → severe overfitting
    \item CNN: Training accuracy 95\%, test 92.6\% (2.4\% gap) → well-controlled
    \item Random Forest: Ensemble averaging prevented overfitting entirely
\end{itemize}

\textbf{5. Unexpected Finding:}

MLP performing \textit{worse} than Random Forest was initially surprising but makes theoretical sense: flattening images destroys spatial structure so completely that even deep learning cannot overcome it without astronomical data quantities. This emphasizes that architectural inductive bias is not just an optimization—it's fundamental to learning from limited data.

\section{Conclusion}

\subsection{Summary of Main Findings}

This study systematically compared three machine learning algorithms on the CIFAR-10 image classification task, revealing fundamental insights about algorithm selection, architectural inductive biases, and the relationship between model complexity and performance.

\textbf{Key Findings:}

\textbf{1. CNN Superiority:} The Convolutional Neural Network achieved 92.58\% test accuracy with test-time augmentation, outperforming Random Forest (55.46\%) by 37\% and MLP (38.68\%) by 54\%. This demonstrates that specialized architectures designed for spatial data dramatically outperform generic algorithms, even when those generic algorithms are theoretically capable of learning arbitrary functions.

\textbf{2. Importance of Inductive Bias:} Random Forest with manually engineered features (HOG, color histograms) outperformed MLP despite being conceptually simpler. This counterintuitive result emphasizes that the right problem representation matters more than model sophistication. CNNs succeed by encoding spatial inductive biases directly into the architecture (convolution, pooling, local connectivity).

\textbf{3. Accuracy-Time Trade-off:} CNN required 150× longer training than Random Forest (2771.46s vs 18.72s) but delivered 37\% higher accuracy. For production deployment, this one-time training cost is justified by superior predictions and reasonable inference time (5.2ms/image). Random Forest offers a fast baseline (2.3ms inference) when 55\% accuracy suffices.

\textbf{4. Hyperparameter Sensitivity:} Neural networks showed greater sensitivity to hyperparameters than Random Forest, with MLP being most sensitive (3.2\% performance range) and Random Forest most stable (0.4\% range). This reflects the challenge of optimizing high-dimensional parameter spaces versus simple tree-based ensembles.

\textbf{5. Practical Techniques Matter:} Modern deep learning practices (data augmentation, batch normalization, learning rate scheduling, dropout) were essential for CNN success:
\begin{itemize}
    \item Data augmentation alone improved CNN accuracy by ~5\%
    \item Test-time augmentation added another 0.7\%
    \item Batch normalization enabled stable training of deep architectures
    \item ReduceLROnPlateau prevented premature convergence
\end{itemize}

\textbf{6. Class-Specific Challenges:} All algorithms struggled with visually similar classes (cat/dog, bird/airplane, automobile/truck) and excelled on distinctive classes (ship, frog, automobile). CNN's superior performance stemmed from learning discriminative features that generalize across variations, while traditional methods relied on class-specific feature templates.

\textbf{Considering Runtime and Interpretability:}

\textbf{Runtime:} While CNN has the longest training time (46 minutes), this is a one-time cost. For inference, all methods are real-time capable (<30ms/image). Random Forest offers the best training-inference balance for rapid prototyping, while CNN is ideal when accuracy is paramount.

\textbf{Interpretability:} Random Forest provides clear advantages:
\begin{itemize}
    \item Feature importance rankings show which features (HOG orientations, color bins) drive decisions
    \item Tree paths can be traced to explain individual predictions
    \item Useful for domains requiring model transparency (medical, legal)
\end{itemize}

Neural networks (MLP, CNN) are black boxes, though CNN filters can be visualized to show learned features (edges in layer 1, textures in layer 2, parts in layer 3). For applications prioritizing interpretability over accuracy, Random Forest remains competitive.

\subsection{Study Limitations}

\textbf{1. Dataset Scope:} CIFAR-10's low resolution (32×32 pixels) and limited diversity (10 classes, 50K training images) may not generalize to real-world applications with:
\begin{itemize}
    \item Higher resolutions requiring deeper networks
    \item More classes requiring greater model capacity
    \item Imbalanced class distributions requiring specialized techniques
    \item Domain-specific challenges (medical imaging, satellite imagery)
\end{itemize}

\textbf{2. Computational Constraints:} Limited GPU resources constrained:
\begin{itemize}
    \item Maximum network depth and width explored
    \item Extent of hyperparameter search (manual grid vs exhaustive search)
    \item Number of training epochs (100 vs 200+ in state-of-the-art)
    \item Advanced architectures (ResNet, DenseNet, Vision Transformers)
\end{itemize}

\textbf{3. Hyperparameter Search Strategy:} Manual grid search based on intuition may have missed optimal configurations. More sophisticated approaches (Bayesian optimization, neural architecture search) could potentially improve all models, though the relative ranking would likely remain unchanged.

\textbf{4. Feature Engineering Scope:} Random Forest used standard HOG and color features. More sophisticated engineered features (SIFT, SURF, learned dictionary features) might improve performance, though unlikely to close the 37\% gap to CNN.

\textbf{5. Single Dataset Evaluation:} Conclusions are based on CIFAR-10 alone. Other image classification tasks (CIFAR-100, ImageNet, domain-specific datasets) might show different relative performance, though CNN's fundamental advantages for spatial data should persist.

\textbf{6. No Ensemble Methods for Neural Networks:} We evaluated single CNN models. Ensembling multiple CNNs (common in competitions) could push accuracy to 94-95\% but increases computational cost proportionally.

\subsection{Future Work}

\textbf{Concrete suggestions addressing study limitations:}

\textbf{1. Advanced CNN Architectures:} Implement modern architectures that have shown superior performance on CIFAR-10:
\begin{itemize}
    \item \textbf{ResNet (Residual Networks):} Skip connections enable training very deep networks (50-100+ layers) by solving vanishing gradient problems. Expected improvement: 93\% → 95\%
    \item \textbf{DenseNet:} Dense connections between layers improve feature reuse and gradient flow. Expected improvement: 93\% → 94.5\%
    \item \textbf{EfficientNet:} Compound scaling of depth, width, and resolution for optimal accuracy-efficiency trade-off
\end{itemize}

\textbf{Justification:} These architectures have theoretical advantages (gradient flow, feature reuse) proven on CIFAR-10 benchmarks.

\textbf{2. Transfer Learning:} Pretrain CNNs on ImageNet (1.2M images, 1000 classes) then fine-tune on CIFAR-10:
\begin{itemize}
    \item Leverages large-scale feature learning from ImageNet
    \item Reduces training time (only fine-tune upper layers)
    \item Expected improvement: 93\% → 94\%
    \item Particularly effective when training data is limited
\end{itemize}

\textbf{Justification:} Transfer learning is standard practice in computer vision when training data is scarce relative to model capacity.

\textbf{3. Advanced Regularization Techniques:}
\begin{itemize}
    \item \textbf{Mixup:} Train on convex combinations of image pairs with interpolated labels; reduces overfitting and improves calibration
    \item \textbf{CutMix:} Similar to mixup but cuts and pastes image regions; forces model to attend to all spatial locations
    \item \textbf{AutoAugment:} Learn optimal data augmentation policies via reinforcement learning
\end{itemize}

\textbf{Justification:} These techniques have demonstrated consistent improvements on CIFAR-10 in published literature, adding 1-2\% accuracy.

\textbf{4. Ensemble Methods:}
\begin{itemize}
    \item Train 5-10 CNN models with different initializations and architectures
    \item Combine predictions via averaging or stacking
    \item Expected improvement: 93\% → 94.5-95\%
\end{itemize}

\textbf{Justification:} Ensembles reduce variance and are standard in competitions, though they proportionally increase computational cost.

\textbf{5. Cross-Dataset Evaluation:} Test all three algorithms on:
\begin{itemize}
    \item \textbf{CIFAR-100:} 100 classes instead of 10; tests fine-grained discrimination
    \item \textbf{SVHN:} Street View House Numbers; tests generalization to different image statistics
    \item \textbf{Tiny ImageNet:} 200 classes, higher resolution; tests scaling to larger problems
\end{itemize}

\textbf{Justification:} Validates that conclusions generalize beyond a single dataset and reveals any dataset-specific biases in our findings.

\textbf{6. Interpretability Analysis:}
\begin{itemize}
    \item Visualize CNN learned filters and activation maps (Grad-CAM)
    \item Analyze which image regions drive predictions
    \item Compare interpretability with Random Forest feature importance
    \item Quantify accuracy-interpretability trade-off
\end{itemize}

\textbf{Justification:} Understanding what models learn is crucial for trust in deployment, especially in high-stakes applications. This would provide practical guidance for algorithm selection when interpretability is required.

\textbf{7. Computational Efficiency Optimization:}
\begin{itemize}
    \item Investigate model compression (pruning, quantization) to reduce CNN inference time
    \item Knowledge distillation: Train small "student" CNN to mimic large "teacher" CNN
    \item Neural Architecture Search (NAS) to find optimal accuracy-efficiency trade-offs
\end{itemize}

\textbf{Justification:} Addresses the practical challenge of deploying high-accuracy models on resource-constrained devices (mobile, embedded systems).

\section{Reflection}

\textbf{[Individual Reflection - To be completed by each group member]}

\textit{The most important insight I gained from this assignment is the profound impact of architectural inductive biases on learning from limited data. Before this assignment, I understood theoretically that CNNs were superior for images, but implementing and comparing all three algorithms revealed the magnitude of this advantage and the underlying reasons.}

\textit{Specifically, I was surprised that MLP performed worse than Random Forest despite being a "more powerful" model. This drove home the lesson that model capacity is useless without appropriate inductive biases—the MLP's ability to approximate any function is wasted when it must learn position-specific features for every pixel location. Random Forest, with carefully engineered HOG and color features, encodes more relevant prior knowledge about visual data, enabling better performance despite conceptual simplicity.}

\textit{The hyperparameter tuning process was particularly educational. I learned that finding optimal hyperparameters is not just about grid search, but about understanding the underlying mechanisms—why lower learning rates stabilize training, why dropout prevents co-adaptation, why progressive channel increase in CNNs maintains representational capacity. This systematic experimentation and analysis developed my intuition for designing and debugging deep learning systems.}

\textit{Furthermore, implementing data augmentation and test-time augmentation demonstrated how to artificially increase dataset size and improve robustness. Seeing augmentation add 5\% accuracy reinforced that in practice, engineering techniques often matter as much as algorithmic sophistication.}

\textit{Finally, this assignment integrated knowledge across the entire course—decision trees, ensemble methods, neural networks, CNNs, optimization, regularization—into a cohesive comparative study. Understanding how these concepts relate and interact is more valuable than memorizing individual techniques. This holistic perspective will guide my future work in machine learning, emphasizing that successful ML requires not just implementing algorithms, but understanding when and why to apply each approach.}

\section{References}

\begin{thebibliography}{99}

\bibitem{krizhevsky2009learning}
Krizhevsky, A., \& Hinton, G. (2009). \textit{Learning multiple layers of features from tiny images}. Technical report, University of Toronto.

\bibitem{breiman2001random}
Breiman, L. (2001). Random forests. \textit{Machine Learning}, 45(1), 5-32.

\bibitem{dalal2005histograms}
Dalal, N., \& Triggs, B. (2005). Histograms of oriented gradients for human detection. In \textit{IEEE Computer Society Conference on Computer Vision and Pattern Recognition} (Vol. 1, pp. 886-893).

\bibitem{perez2017effectiveness}
Perez, L., \& Wang, J. (2017). The effectiveness of data augmentation in image classification using deep learning. \textit{arXiv preprint arXiv:1712.04621}.

\bibitem{rumelhart1986learning}
Rumelhart, D. E., Hinton, G. E., \& Williams, R. J. (1986). Learning representations by back-propagating errors. \textit{Nature}, 323(6088), 533-536.

\bibitem{hornik1989multilayer}
Hornik, K., Stinchcombe, M., \& White, H. (1989). Multilayer feedforward networks are universal approximators. \textit{Neural Networks}, 2(5), 359-366.

\bibitem{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., \& Haffner, P. (1998). Gradient-based learning applied to document recognition. \textit{Proceedings of the IEEE}, 86(11), 2278-2324.

\bibitem{hubel1968receptive}
Hubel, D. H., \& Wiesel, T. N. (1968). Receptive fields and functional architecture of monkey striate cortex. \textit{The Journal of Physiology}, 195(1), 215-243.

\bibitem{ioffe2015batch}
Ioffe, S., \& Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. In \textit{International Conference on Machine Learning} (pp. 448-456).

\bibitem{srivastava2014dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., \& Salakhutdinov, R. (2014). Dropout: A simple way to prevent neural networks from overfitting. \textit{Journal of Machine Learning Research}, 15(1), 1929-1958.

\bibitem{kingma2014adam}
Kingma, D. P., \& Ba, J. (2014). Adam: A method for stochastic optimization. \textit{arXiv preprint arXiv:1412.6980}.

\bibitem{simonyan2014very}
Simonyan, K., \& Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. \textit{arXiv preprint arXiv:1409.1556}.

\end{thebibliography}

\end{document}
